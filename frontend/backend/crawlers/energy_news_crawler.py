# crawlers/energy_news_crawler.py
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
import json
import time
from datetime import datetime

ENERGY_NEWS_URL = (
    "https://www.moeaea.gov.tw/ECW/populace/news/News.aspx"
    "?kind=1&menu_id=41"
)

OUTPUT_PATH = "energy_news_cache.json"

def crawl_energy_news():
    # ===== å•Ÿå‹•ç€è¦½å™¨ =====
    options = Options()
    options.add_argument("--headless=new")  # è‹¥æŠ“ä¸åˆ°å¯å…ˆæ”¹æˆ False
    options.add_argument("--disable-gpu")
    options.add_argument("--no-sandbox")

    driver = webdriver.Chrome(
        service=Service(ChromeDriverManager().install()),
        options=options
    )

    print("ğŸŒ é–‹å•Ÿèƒ½æºç½²å…¬å‘Šé â€¦")
    driver.get(ENERGY_NEWS_URL)
    time.sleep(6)  # â³ ç­‰ JS è¼‰å…¥ï¼ˆé‡è¦ï¼‰

    items = []

    # ===== æŠ“æ‰€æœ‰ã€Œå…¬å‘Šé€£çµã€ï¼ˆç©©å®šåšæ³•ï¼‰=====
    links = driver.find_elements(
        By.XPATH,
        "//a[contains(@href, 'News.aspx')]"
    )

    for a in links:
        title = a.text.strip()
        link = a.get_attribute("href")

        # éæ¿¾ç„¡æ•ˆé …ç›®
        if not title:
            continue
        if "menu_id" not in link:
            continue

        items.append({
            "title": title,
            "link": link
        })

        if len(items) >= 5:
            break

    driver.quit()

    data = {
        "source": "ç¶“æ¿Ÿéƒ¨èƒ½æºç½²",
        "synced_at": datetime.now().isoformat(),
        "items": items
    }

    with open(OUTPUT_PATH, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

    print(f"âœ… åŒæ­¥å®Œæˆï¼Œå…± {len(items)} å‰‡å…¬å‘Š")

if __name__ == "__main__":
    crawl_energy_news()
