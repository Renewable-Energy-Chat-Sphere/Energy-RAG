import os
import io
import re
import time
from collections import defaultdict, deque
from pathlib import Path
from dotenv import load_dotenv
from flask import Flask, render_template, request, jsonify
import pandas as pd  # âœ… Table Support

load_dotenv()

from pipelines.rag_web import qa_over_web
from pipelines.rag_pdf import qa_over_pdf
from pipelines.rag_av import qa_over_av

# âœ… Table Support - optional: OpenAI
try:
    from openai import OpenAI

    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    openai_client = OpenAI(api_key=OPENAI_API_KEY) if OPENAI_API_KEY else None
except Exception:
    openai_client = None

app = Flask(__name__)
app.config["MAX_CONTENT_LENGTH"] = 512 * 1024 * 1024  # 512MB


@app.route("/")
def index():
    return render_template("index.html")


# =========================
# RAG routes
# =========================
@app.route("/ask_web", methods=["POST"])
def ask_web():
    data = request.get_json(force=True)
    question = (data.get("question") or "").strip()
    url = (data.get("url") or "").strip() or None
    if not question:
        return jsonify({"error": "question is required"}), 400
    answer, sources = qa_over_web(question, url=url)
    return jsonify({"answer": answer, "sources": sources})


@app.route("/ask_pdf", methods=["POST"])
def ask_pdf():
    question = (request.form.get("question") or "").strip()
    file = request.files.get("file")
    if not question:
        return jsonify({"error": "question is required"}), 400
    if not file:
        return jsonify({"error": "PDF file is required"}), 400
    answer, sources = qa_over_pdf(question, file)
    return jsonify({"answer": answer, "sources": sources})


@app.route("/ask_av", methods=["POST"])
def ask_av():
    question = (request.form.get("question") or "").strip()
    file = request.files.get("file")
    if not question:
        return jsonify({"error": "question is required"}), 400
    if not file:
        return jsonify({"error": "audio/video file is required"}), 400
    answer, sources = qa_over_av(question, file)
    return jsonify({"answer": answer, "sources": sources})


# =========================
# âœ… Table Support
# =========================
def df_to_markdown(df: pd.DataFrame, max_rows=30, max_cols=15):
    df2 = df.copy()
    if df2.shape[0] > max_rows:
        df2 = df2.head(max_rows)
    if df2.shape[1] > max_cols:
        df2 = df2.iloc[:, :max_cols]
    df2 = df2.convert_dtypes()
    return df2.to_markdown(index=False)


def read_table_file(file_storage):
    content = file_storage.read()
    bio = io.BytesIO(content)
    name = (file_storage.filename or "").lower()

    if name.endswith((".xlsx", ".xlsm", ".xltx", ".xltm", ".xls")):
        xls = pd.ExcelFile(bio)
        sheets = {}
        for sheet_name in xls.sheet_names:
            sheets[sheet_name] = xls.parse(sheet_name)
        return sheets
    elif name.endswith(".csv") or name.endswith(".txt"):
        return {"Sheet1": pd.read_csv(bio, encoding="utf-8", engine="python")}
    elif name.endswith((".tsv", ".tab")):
        return {"Sheet1": pd.read_csv(bio, sep="\t", encoding="utf-8", engine="python")}
    else:
        # fallback
        try:
            return {"Sheet1": pd.read_csv(bio, engine="python")}
        except Exception:
            raise ValueError("ä¸æ”¯æ´çš„æª”æ¡ˆæ ¼å¼ï¼Œè«‹ä¸Šå‚³ .xlsx/.xls/.csv/.tsv")


def build_context_markdown(sheets: dict, max_chars=12000):
    parts = []
    for name, df in sheets.items():
        md = df_to_markdown(df)
        parts.append(f"### Sheet: {name}\n\n{md}\n")
    ctx = "\n\n".join(parts)
    if len(ctx) > max_chars:
        ctx = ctx[:max_chars] + "\n\n...(å·²æˆªæ–·)..."
    return ctx


def simple_stats(sheets: dict):
    info = []
    for name, df in sheets.items():
        cols = list(map(str, df.columns.tolist()))
        info.append(
            {
                "sheet": name,
                "shape": [int(df.shape[0]), int(df.shape[1])],
                "columns_sample": cols[:15],
            }
        )
    return info


def llm_answer_from_tables(question: str, ctx_md: str):
    if not openai_client:
        return None
    prompt = f"""æ ¹æ“šä¸‹æ–¹è¡¨æ ¼å…§å®¹ï¼ˆMarkdownï¼‰ï¼Œå›ç­”ä½¿ç”¨è€…çš„å•é¡Œã€‚
è¡¨æ ¼å…§å®¹ï¼š
{ctx_md}

å•é¡Œï¼š
{question}
"""
    try:
        resp = openai_client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.2,
        )
        return resp.choices[0].message.content.strip()
    except Exception as e:
        return f"(LLM å›ç­”å¤±æ•—) {e}"


@app.route("/ask_table", methods=["POST"])
def ask_table():
    question = (request.form.get("question") or "").strip()
    file = request.files.get("file")
    if not question:
        return jsonify({"error": "question is required"}), 400

    if not file:
        return jsonify({"error": "table file is required"}), 400

    try:
        sheets = read_table_file(file)
        stats = simple_stats(sheets)
        ctx_md = build_context_markdown(sheets)

        answer = llm_answer_from_tables(question, ctx_md)
        if not answer:
            md_list = []
            for name, df in sheets.items():
                md_list.append(
                    f"### {name}\nè¡Œæ•¸Ã—åˆ—æ•¸ï¼š{df.shape[0]}Ã—{df.shape[1]}\n\n{df_to_markdown(df)}"
                )
            answer = "æœªè¨­å®š OPENAI_API_KEYï¼Œå›å‚³æ‘˜è¦ï¼š\n\n" + "\n\n---\n\n".join(
                md_list
            )

        return jsonify({"answer": answer, "sources": stats})
    except Exception as e:
        return jsonify({"error": str(e)}), 500


# =========================
# ğŸ¤– Generic Chatbot + è¼•é‡æ™ºæ…§è·¯ç”±ï¼ˆå«ç¶²å€è‡ªå‹•æ”¹ç”¨ /ask_webï¼‰
# =========================

# å°è©±æ­·å²ï¼ˆå¤š sessionï¼‰ï¼Œåƒ…åœ¨å–®æ©Ÿé–‹ç™¼æ™‚ä½¿ç”¨ï¼›æ­£å¼ç’°å¢ƒè«‹æ”¹ç‚º DB / Redis
CHAT_MAX_MESSAGES = 30
CHAT_SESSIONS: dict[str, deque] = defaultdict(lambda: deque(maxlen=CHAT_MAX_MESSAGES))

# åµæ¸¬è¨Šæ¯ä¸­æ˜¯å¦å« URL
URL_RE = re.compile(r"(https?://[^\s]+)", re.IGNORECASE)


def _build_messages(session_id: str, user_text: str, system_prompt: str | None = None):
    msgs = []
    if system_prompt:
        msgs.append({"role": "system", "content": system_prompt})
    for role, content in CHAT_SESSIONS[session_id]:
        msgs.append({"role": role, "content": content})
    msgs.append({"role": "user", "content": user_text})
    return msgs


def _store_turn(session_id: str, user_text: str, assistant_text: str):
    CHAT_SESSIONS[session_id].append(("user", user_text))
    CHAT_SESSIONS[session_id].append(("assistant", assistant_text))


@app.route("/chat", methods=["POST"])
def chat():
    """
    JSON body:
    {
      "session_id": "abc123",        # å¯é¸ï¼Œé è¨­ "default"
      "user": "ä½ çš„å•é¡Œæ–‡å­—",           # å¿…å¡«
      "system": "ä½ æ˜¯åŠ©æ•™â€¦",            # å¯é¸
      "model": "gpt-4o-mini",        # å¯é¸
      "rag_auto": true               # å¯é¸ï¼›true æ™‚è‹¥åµæ¸¬åˆ° URL æœƒæ”¹èµ° qa_over_web
    }
    """
    data = request.get_json(force=True, silent=False) or {}
    session_id = (data.get("session_id") or "default").strip() or "default"
    user_text = (data.get("user") or "").strip()
    system_prompt = (data.get("system") or "").strip() or None
    model = (data.get("model") or "gpt-4o-mini").strip() or "gpt-4o-mini"
    rag_auto = bool(data.get("rag_auto", True))

    if not user_text:
        return jsonify({"error": "user is required"}), 400

    # 1) è¼•é‡æ™ºæ…§è·¯ç”±ï¼šè¨Šæ¯å« URL â†’ ç”¨ä½ ç¾æˆçš„ RAG Web
    if rag_auto:
        m = URL_RE.search(user_text)
        if m:
            url = m.group(1)
            # å¾åŸå§‹è¨Šæ¯æ‹¿æ‰ urlï¼Œç•¶æˆ question
            question_only = user_text.replace(url, "").strip() or "è«‹æ ¹æ“šç¶²å€å…§å®¹å›ç­”ã€‚"
            try:
                answer, sources = qa_over_web(question_only, url=url)
                _store_turn(session_id, user_text, answer)
                return jsonify(
                    {
                        "answer": answer,
                        "sources": sources,
                        "session_id": session_id,
                        "model": "rag_web",
                        "uses_openai": (
                            False if not openai_client else True
                        ),  # RAG è£¡é¢è‹¥ä¹Ÿç”¨åˆ° LLMï¼Œå‰‡ä»å¯èƒ½ True
                    }
                )
            except Exception as e:
                # ä¸ä¸­æ–·ï¼šæ”¹èµ°ä¸€èˆ¬èŠå¤©
                fallback_note = f"(ç¶²å€è™•ç†å¤±æ•—ï¼Œæ”¹ç”¨ä¸€èˆ¬èŠå¤©) {e}\n\n"
                user_text = (
                    fallback_note + user_text
                )  # å¸¶å…¥è¨Šæ¯ä¸­ï¼Œè®“æ¨¡å‹çŸ¥é“å‰›å‰›ç™¼ç”Ÿä»€éº¼

    # 2) ä¸€èˆ¬èŠå¤©ï¼šæœ‰ OpenAI å°±ç”¨ï¼Œæ²’æœ‰å°±é›¢ç·š fallback
    if openai_client:
        try:
            messages = _build_messages(session_id, user_text, system_prompt)
            resp = openai_client.chat.completions.create(
                model=model,
                messages=messages,
                temperature=0.3,
            )
            assistant_text = (resp.choices[0].message.content or "").strip()
            if not assistant_text:
                assistant_text = "ï¼ˆæ¨¡å‹æ²’æœ‰å›å‚³å…§å®¹ï¼‰"
        except Exception as e:
            assistant_text = (
                f"(LLM å¤±æ•—ï¼Œæ”¹ç”¨é›¢ç·šå›è¦†) {e}\n\n"
                f"ä½ å‰›æ‰èªªï¼š{user_text}\n"
                f"æš«æ™‚å»ºè­°ï¼šç¢ºèª OPENAI_API_KEY è¨­å®šæˆ–ç¨å¾Œå†è©¦ã€‚"
            )
    else:
        ts = time.strftime("%Y-%m-%d %H:%M:%S")
        assistant_text = (
            "ï¼ˆé›¢ç·šæ¨¡å¼ï¼‰æˆ‘ç›®å‰ç„¡æ³•å­˜å–é›²ç«¯æ¨¡å‹ï¼Œä½†æˆ‘å·²æ”¶åˆ°ä½ çš„è¨Šæ¯ã€‚\n"
            f"æ™‚é–“ï¼š{ts}\n\n"
            f"ä½ èªªçš„æ˜¯ï¼š{user_text}\n"
            "å¯ä»¥å˜—è©¦ï¼š\n"
            "1) è¨­å®š OPENAI_API_KEY å¾Œé‡è©¦ï¼›\n"
            "2) è‹¥è¦é‡å°ç¶²å€æˆ–æª”æ¡ˆæå•ï¼Œæ”¹ç”¨ /ask_webã€/ask_pdfã€/ask_avã€/ask_tableã€‚"
        )

    _store_turn(session_id, user_text, assistant_text)

    return jsonify(
        {
            "answer": assistant_text,
            "session_id": session_id,
            "history_len": len(CHAT_SESSIONS[session_id]),
            "model": model,
            "uses_openai": bool(openai_client),
        }
    )


# =========================

if __name__ == "__main__":
    app.run(host="127.0.0.1", port=8000, debug=True)
